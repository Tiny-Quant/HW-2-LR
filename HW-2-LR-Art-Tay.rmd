---
title: "Linear Regression"
author: "Art Tay"
output: pdf_document
header-includes:
    - \usepackage{caption}
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
knitr::opts_knit$set(eval.after = "fig.cap")
# Flattens plot to be images
knitr::opts_chunk$set(dev = "png", dpi = 100)

# Libraries
library(tidyverse)
```

\captionsetup[table]{labelformat = empty}
\captionsetup[figure]{labelfont = bf}

## Gradient Derivation
a)
\begin{align*}
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\dfrac{\partial}{\partial \beta_0}
\left[
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2(\beta_0 + \beta_1 x^{(i)} - y^{(i)}) \cdot 1
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m} \beta_0 +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} -
\frac{1}{m} \sum_{i = 0}^{m} y^{(i)}
\\
&=
\beta_0 + \beta_1 \bar x - \bar y
\end{align*}
b)
\begin{align*}
\dfrac{\partial}{\partial \beta_1}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\dfrac{\partial}{\partial \beta_1}
\left[
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2 x^{(i)}(\beta_0 + \beta_1 x^{(i)} - y^{(i)})
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m}
\left[
\beta_0 x^{(i)}+ \beta_1 {x^{(i)}}^2 - x^{(i)}y^{(i)}
\right]
\\
&=
\beta_0 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m}  {x^{(i)}}^2 -
\frac{1}{m} \sum_{i = 0}^{m} x^{(i)}y^{(i)}
\\
&=
\beta_0 \bar x +
\beta_1 \bar{x^2} -
\bar {xy}
\end{align*}

## Linear Regression by Gradient Decent
```{r Problem Statement 2, include = F}
# Linear regression by gradient descent in R
# The purpose of this exercise is to test what is above and implement that in R.
# Steps:
# a) Generate some random data for X and Y (use rnorm function in R)
# b) Check linearity by running lm(y~x) and get the coefficient
# c) Build Cost and gradient descent function
# d) The output of betas (ùõΩ0  + ùõΩ1) should be equal to the coefficients above
```

```{r a) Random Data Generation}
# Generates linear data with normal residuals
set.seed(123)
x <- rnorm(n = 30)

epsilon <- rnorm(n = 30)

y <- 5*x + 1 + epsilon
```

```{r Plots generated data, include = F}
plot(x, y)
```

```{r b) Checks linearity and coefficients use lm}
summary(lm(y ~ x))
```

```{r c) Defines a function for calculating cost function}
# Calculates the mean squared error for a simple linear regression model.
# @param x - a vector of the explainatory variable.
# @param y - a vector of the response variable.
# @param beta_0 - the intercept value for the current SLR model.
# @param beta_1 - the slope value for the current SLR model.
# @return - sum total mean squared error (y_hat - y)^2
slr_mse <- function(x, y, beta_0, beta_1){
    cost <- ((beta_1 * x + beta_0) - y)^2
    return(sum(cost))
}
```

```{r c) Calculates betas for SLR by gradient descent}
# Calculates the slope and intercept values for SLR
# or simple linear regression.
# @param x - a vector of the explanatory variable.
# @param y - a vector of the response variable.
# @param alpha - the learning rate.
# @return betas - a vector containing the calculated betas.
slr_gradient_desc <- function(x, y, alpha){

    # Summary statistic calculations.
    # Helps to calculate the gradient faster.
    x_bar <- mean(x)
    y_bar <- mean(y)
    xy_bar <- mean(x*y)
    x_sqbar <- mean(x^2)

    # initial guess for beta_0 and beta_1.
    beta_0 <- y_bar
    beta_1 <- 0

    # A counter to determine is the error is unchanging.
    # This is the Loop-Control-Variable (LCV).
    count_same <- 0

    # Iterate 1000 times or until the cost remains unchanged for 10 iterations.
    for(i in 1:1000){

        # Stop the loop if the LCV >= 10.
        if(count_same >= 10){
            break
        }

        # Cost prior to beta adjustment.
        cost_start <- slr_mse(x, y, beta_0, beta_1)

        # Calculate gradient values.
        g_0 <- beta_0 + (beta_1 * x_bar) - y_bar
        g_1 <- (beta_0 * x_bar) + (beta_1 * x_sqbar) - xy_bar

        # Update betas.
        beta_0 <- beta_0 - (alpha * g_0)
        beta_1 <- beta_1 - (alpha * g_1)

        # If the cost is unchanged add 1 to the LCV.
        if(cost_start == slr_mse(x, y, beta_0, beta_1)){
            count_same <- count_same + 1
        }
    }

    return(c(beta_0 = round(beta_0, 4),
             beta_1 = round(beta_1, 4),
             iterations = i))
}
```

```{r d) Output calculated beta_0 and beta_1}
slr_gradient_desc(x, y, alpha = 0.1)
```

\pagebreak

## Linear Model on Airbnb Data
### Introduction
Airbnb, Inc. is a software company that operates a marketplace for people to
rent their properties on an ad hoc basis
[(\textcolor{blue}{Wikipedia})](https://en.wikipedia.org/wiki/Airbnb).
The specific dataset addressed in this report comes from Inside Airbnb,
"a mission driven project that provides data \dots about Airbnb's
impact on residential communities."
[(\textcolor{blue}{Website})](http://insideairbnb.com/about/).
The data set contains 12 predictive variables: 4 categorical and
8 numeric. All of the roughly 40,000 listings in this particular
dataset were located in New York City (Tay, 2022).

The ultimate goal of this analysis was to develope the most predictive
linear model of a particular listing's price.

### Methodology
The data was initial split into training and testing data (70% and 30% of the raw
data respectively). The Training data was analyzed and cleaned separately to
avoid data leakage that might bias the test error rate. The ID variables were
removed as predictor and the factor variables were check for empty strings.
All empty strings in factor variables were recoded as `NA` as were any `0` values
in `price`, `minimum_nights`, `longitude`, and `latitude`. `NA` values for the
review type variables were all associated with properties that had never been
reviewed, so `NA` values in `reviews_per_month` were assigned 0. Furthermore,
the year was extracted from `last_review` and a level of `none` was added for
any empty cells. This was put into a new feature and called `last_review_year`.

Linear models are very sensitive to non-normality in the response variable.
While there are no explicit parametric assumptions about the distributions of
the predictor, skewness can increase model error.
```{r skewness plot, echo = F, fig.cap = caption}
#| fig.align = "center",
#| out.height = '90%',
#| out.width = '90%'
load(file = "Figures/plot_skew.rds")

plot_skew

caption <- "Histograms of raw numeric features before any transformations.
    Most distribution are clearly right skewed"
```

Therefore, the response variable, `price`, was log transformed, and all other
numeric predictors were transformed via the Yeos-Johnson transformation.
Furthermore all numeric predictor were centered and scaled. Centering can
address issues related to multicollinearity and scaling is a requirement of
most imputation algorithms. All nominal predictors were transformed into $n-1$
dummy variables and 11 missing price values were imputed using k-nearest-neighbors
with $k = 10$. Afterwards, near-zero-variance and highly correlated predictors
were dropped from the pool. Given the large sample size this step was not strictly
necessary; however, it can address overfitting and multicollinearity.

Another important assumption of linear regression is a linear relationship
between all predictors and y.
```{r linearity plot, echo = F, fig.cap = caption}
#| fig.align = 'center',
#| out.height = '100%',
#| out.width = '100%'

load(file = "Figures/plot_scatter.rds")

plot_scatter

caption <- "Scatter plot of transformed numeric predictors against log(price).
Note that the x-axis is center and scaled relative to the predictor."
```
None of the predictors share a strong linear relationship with price; however,
there are methods for including nonlinear relationships into a linear regression
model. Frank Harrell in his book \emph{Regression Modeling Strategies},
recommends treating all numeric predictors are natural cubic splines.
Natural cubic splines are piecewise third degree
polynomial transformations of predictors with added smoothness restrictions.
They can then be pasted to the linear model via 3\* basis functions per variable
(marked by _ns\#).

\* Not all variables produced 3 uncorrelated basis functions.
High correlated predictors were dropped again after the spline transformation which
left some predictors defined by 2 basis functions. The non-linear effect is
still captured.

\pagebreak

### Results
The final model had 32 $\beta$s with almost all of them being statistically
significant.
```{r diagnosis plot, echo = F, fig.cap = caption}
#| fig.align = 'center',
#| out.height = '60%',
#| out.width = '60%'

load(file = "Figures/mlr_fit_lm.rds")

par(mfrow = c(1,2))

plot(mlr_fit_lm, which = 1:2)

caption <- "Diagnostic plots from the fit multiple linear regression model.
The assumptions of homoscedasticity and normality are clearly violated"
```

```{r outlier plots, echo = F, fig.cap = caption}
#| fig.align = 'center',
#| out.height = '60%',
#| out.width = '60%'

load(file = "Figures/inful.rds")

inful

caption <- "Analysis of potential outliers from the MLR model fit.
Although there are some high leverage points, none of them are extremely
influential to the model based on Cook's distance. Removing points is unlikely
to improve the model fit."
```

Unfortunately, because the model assumption are not met, no statistical inference
can be draw from the model; however, the prediction may still be accurate.

\pagebreak

### Conclusions
```{r metric table, echo = F, fig.cap = NULL}
#| fig.align = 'center'

load(file = "Figures/metric_table.rds")

metric_table
```

```{r variable importance plot, echo = F, fig.cap = caption}
#| fig.align = 'center'

load(file = "Figures/vip_plot.rds")

vip_plot

caption <- "Top predictor variables by importance."
```

Based on adjusted $R^2$, the linear model explains roughly 50% of the variation
in price for both the training and testing data. There are no significant drops
in model metrics on the test data, which indicates that the model is not
over fit. The ratio metrics indicate that the majority of predictions are
within $\pm10\%$ of the true price with almost all predictions within $\pm20\%$.
Despite the extreme non-linearity of the data and the unequal variance of the
residuals, the model prediction are usable. Additional predictive power
may be achieved by transitioning to a tree based model; however, the data
appears not to explain the vast majority of the variation in prices across
listings.

\pagebreak

## Code for Linear Model
```{r modeling libraries}
# Libraries
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
```

```{r notes on model building, include = F}
# Skewness in the predictors does matter as long as the residuals are
# normally distributed. This usually only concerns y; however,
# skewness in x can lead to influential points.
```

### Modeling Building
```{r Data import}
# Import raw data.
data_raw <- read.csv(file = "AB_NYC_2019.csv", stringsAsFactors = T,
                     header = T)
```

```{r Train/Test Split}
# 70/30 train/test data split
# Data was split prior to cleaning to prevent data leakage.
set.seed(123)
data_split <- initial_split(data_raw, prop = 0.7)

train_data <- training(data_split)
test_data <- testing(data_split)
```

```{r Initialize a recipe variable}
# Define a cleaning recipe from the data.
# Assigns price the role of response.
# Recipe is used to create a pipline that can works with new raw data
# assuming that the format matches the training data.
cleaning_recipe <- recipe(price ~ ., data = train_data)
```

```{r Add remove id variable step to recipe}
cleaning_recipe <- cleaning_recipe %>% step_rm(id, host_id, name, host_name)
```

```{r Add recode missing values to recipe}
cleaning_recipe <- cleaning_recipe %>%
    step_mutate(
        # numeric recodes
        latitude = ifelse(latitude == 0, NA, latitude),
        longitude = ifelse(longitude == 0, NA, longitude),
        minimum_nights = ifelse(minimum_nights == 0, NA, minimum_nights),

        #Factor recodes
        # It was done using replace to maintain factor coding
        room_type = replace(room_type,
            room_type == "" | room_type == " ", NA),
        neighbourhood_group = replace(neighbourhood_group,
            neighbourhood_group == "" | neighbourhood_group == " ",  NA),
        neighbourhood = replace(neighbourhood,
            neighbourhood == "" | neighbourhood == " ", NA),
        last_review = replace(last_review,
            last_review == "" | room_type == " ", NA)
    )
```

```{r Recode the response separately}
# Skip = T tell the fit to ignore the step if it can't be done.
# Necessary because we assume the test_data does not have the response.
cleaning_recipe <- cleaning_recipe %>%
                   step_mutate(price = ifelse(price == 0, NA, price), skip = T)
```
```{r Feature Engineering}
# Geodist (?)
cleaning_recipe <- cleaning_recipe %>%
    step_mutate(

            # Change NA reviews per month to be 0.
            reviews_per_month = ifelse(is.na(reviews_per_month),
                0, reviews_per_month),

            # Create a new variable last_review_year to reduce dimensionality.
            last_review_year = substring(last_review, 1, 4),

            # Modify NA last_review to be a new level "none".
            last_review_year = replace(last_review_year,
            is.na(last_review_year), "none"),

            # Cast to be factor.
            last_review_year = as.factor(last_review_year),

            # Remove old last review variable
            last_review = NULL
    )
```

```{r Check Skewness}
plot_skew <- train_data %>%
             select(where(is.numeric)) %>%
             select(-c(id, host_id)) %>%
             pivot_longer(cols = everything()) %>%
             ggplot(aes(value)) +
             geom_histogram() +
             facet_wrap(~name, scales = "free_x") +
             theme_bw() +
             labs(x = "", y = "Frequency")

plot_skew

save(plot_skew, file = "Figures/plot_skew.rds")
```

```{r Add log transform and YeoJohnson to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   step_YeoJohnson(all_numeric_predictors()) %>%
                   # price was logged instead of YeoJohnson to maintain
                   # reversibility and interpretability.
                   step_log(price, skip = T)
```

```{r Add Normalize, Dummy Variables, and Imputation to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   step_normalize(all_numeric_predictors()) %>%
                   step_dummy(all_nominal_predictors()) %>%
                   step_impute_knn(everything(), neighbors = 10,
                                   impute_with = imp_vars(everything()),
                                   skip = T)
```

```{r Add filter out NZV and High CORR to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   # removes predictor with less than 10% unique values and
                   # a greater than 95/5 ratio of most prevalent to next most
                   step_nzv(all_predictors()) %>%
                   step_corr(all_numeric_predictors())
```

```{r Bake at substage to check linearity assumption, cache = T}
cleaning_recipe <- cleaning_recipe %>% prep(retain = T, verbose = T)
train_scatter <- bake(cleaning_recipe, new_data = NULL)
```

```{r Check Linearity}
# Matrix Scatterplot
plot_scatter <- train_scatter %>%
                select(-starts_with(c("neighbourhood",
                                      "room_type", "last_review"))) %>%
                pivot_longer(cols = -price) %>%
                ggplot(aes(x = value, y = price)) +
                geom_point() +
                facet_wrap(~name, scales = "free_x") +
                theme_bw() +
                labs(x = "", y = "ln(Price)")

plot_scatter

save(plot_scatter, file = "Figures/plot_scatter.rds")
```

```{r Convert numerics into natural cubic splines}
cleaning_recipe <- cleaning_recipe %>%
                   # makes sure not to accidentally spline your response variable
                   step_ns(-starts_with(c("neighbourhood",
                                "room_type", "last_review", "price")),
                           deg_free = 3) %>%
                   # called again to remove overdetermined splines
                   step_corr(all_numeric_predictors())
```

```{r Summary of MLR Model Build}
mlr_mod <- linear_reg() %>% set_engine("lm")

mlr_wflow <- workflow() %>%
             add_model(mlr_mod) %>%
             add_recipe(cleaning_recipe)

mlr_wflow
```

```{r Summary of MLR Fit}
mlr_fit <- mlr_wflow %>%
           fit(data = train_data)

mlr_fit_lm <- mlr_fit %>% extract_fit_engine()

summary(mlr_fit_lm)

par(mfrow = c(1,2))
plot(mlr_fit_lm, which = 1:2)

save(mlr_fit_lm, file = "Figures/mlr_fit_lm.rds")
```

```{r Check Influential Points}
library(MASS)
library(ggpubr)

##Influential Point Analysis ##

##Calculate Leverage, Studentized Residuals, and Cook's Distance.
Id <- 1:length(train_data$id)
Leverage <- hatvalues(mlr_fit_lm)
StudRes <- studres(mlr_fit_lm)
CookD <- cooks.distance(mlr_fit_lm)

inful_data <- cbind(Id, Leverage, StudRes, CookD)
inful_data <- as.data.frame(inful_data)

##Plots
##Leverage
lev <- ggplot(data = inful_data, aes(x = Id, y = Leverage)) + geom_point() +
            geom_hline(yintercept = 2 * length(mlr_fit_lm$coefficients) /
                length(inful_data$Id), col = "red") +
            labs(x = "Index") +
            theme_bw()

##Studentized Residuals
studres <- ggplot(data = inful_data, aes(x = Id, y = StudRes)) + geom_point() +
                geom_hline(yintercept = 2, col = "red") +
                geom_hline(yintercept = -2, col = "red") +
                labs(y = "Studentized Residuals", x = "Index") +
                theme_bw()

##Cooks distance
cooks <- ggplot(data = inful_data, aes(x = Id, y = CookD)) + geom_point() +
             geom_hline(yintercept = 1, col = "red") +
             labs(y = "Cook's Distance", x = "Index") +
             theme_bw()

inful <- ggarrange(lev, studres, cooks, ncol = 3, nrow = 1)

inful <- annotate_figure(inful, top = text_grob("Influential Point Analysis"))

inful

save(inful, file = "Figures/inful.rds")
```


```{r Back transform the prediction, cache = T}
# Group prediction results raw v. training error v. testing error.
# metric r = Predicted / Actually
# Looking for the mean closest to 1 with the smallest spread.

# Extract transformed pricing variables
train_baked <- bake(prep(cleaning_recipe), new_data = NULL)

# Fresh = T tell the recipe to apply all the steps again
# training = test_data tell the recipe to treat the test_data
# as if it was the training data used in the preprocessing step
# this avoids the modeling problem of steps having to be skipped
# when the response is absent from the dataset.
test_baked <- bake(
    prep(cleaning_recipe, fresh = T, training = test_data, retain = T),
    new_data = NULL)

train_price <- train_baked %>% select(price)
test_price <- test_baked %>% select(price)
train_pred <- predict(mlr_fit, new_data = train_data)
test_pred <- predict(mlr_fit, new_data = test_data)

training_results <- cbind(train_price, train_pred)
testing_results <- cbind(test_price, test_pred)

training_results <- training_results %>%
                    mutate(ratio = price / .pred)

testing_results <- testing_results %>%
                    mutate(ratio = price / .pred)
```

```{r Model evaluation metrics}
train_rss <- sum((training_results$price - training_results$.pred)^2)
test_rss <- sum((testing_results$price - testing_results$.pred)^2)

train_n <- nrow(training_results)
test_n <- nrow(testing_results)
model_p <- length(coef(mlr_fit_lm)) - 1 #subtract 1 to ignore the intercept

train_tss <- var(training_results$price) * train_n
test_tss <- var(testing_results$price) * test_n

train_rsq <- 1 - (train_rss / train_tss)
test_rsq <- 1 - (test_rss / test_tss)

train_adj_rsq <- 1 - (1 - train_rsq) * (train_n - 1) /
                 (train_n - model_p - 1)
test_adj_rsq <- 1 - (1 - test_rsq) * (test_n - 1) /
                 (test_n - model_p - 1)

hist(training_results$ratio)
hist(testing_results$ratio)

train_ratio_mean <- mean(training_results$ratio)
train_ratio_sd <- sd(training_results$ratio)
test_ratio_mean <- mean(testing_results$ratio)
test_ratio_sd <- sd(testing_results$ratio)
```

```{r model metric table}
#model metric table
library(kableExtra)
metric_table <- data.frame(train = c(train_adj_rsq, train_ratio_mean,
                                     train_ratio_sd),
                           test = c(test_adj_rsq, test_ratio_mean,
                                    test_ratio_sd))

colnames(metric_table) <- c("Training Data", "Testing Data")
rownames(metric_table) <- c("Adjusted $R^2$", "Ratio - Mean",
                            "Ratio - Standard Deviation")

caption <- "\\textbf{Table 1: }Training and Testing Model Fit Metrics"

metric_table <- metric_table %>% kbl(format = "latex", booktabs = T,
    longtable = T, escape = F, digits = 3, caption = caption) %>%
    kable_styling(full_width = F)

save(metric_table, file = "Figures/metric_table.rds")
```

```{r Variable importance, cache = T}
library(vip)
vip_plot  <- mlr_fit %>%
             extract_fit_parsnip %>%
             vip() + theme_bw()

vip_plot

save(vip_plot, file = "Figures/vip_plot.rds")
```