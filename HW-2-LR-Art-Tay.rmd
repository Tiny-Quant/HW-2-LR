---
title: "Linear Regression"
author: "Art Tay"
output: pdf_document
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
```

## Gradient Derivation
a)
\begin{align*}
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2(\beta_0 + \beta_1 x^{(i)} - y^{(i)}) \cdot 1
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m} \beta_0 +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} -
\frac{1}{m} \sum_{i = 0}^{m} y^{(i)}
\\
&=
\beta_0 + \beta_1 \bar x - \bar y
\end{align*}
b)
\begin{align*}
\dfrac{\partial}{\partial \beta_1}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\dfrac{\partial}{\partial \beta_1}
\left[
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2 x^{(i)}(\beta_0 + \beta_1 x^{(i)} - y^{(i)})
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m}
\left[
\beta_0 x^{(i)}+ \beta_1 {x^{(i)}}^2 - x^{(i)}y^{(i)}
\right]
\\
&=
\beta_0 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m}  {x^{(i)}}^2 -
\frac{1}{m} \sum_{i = 0}^{m} x^{(i)}y^{(i)}
\\
&=
\beta_0 \bar x +
\beta_1 \bar{x^2} -
\bar {xy}
\end{align*}

## Linear Regression by Gradient Decent
```{r Problem Statement 2, include = F}
# Linear regression by gradient descent in R
# The purpose of this exercise is to test what is above and implement that in R.
# Steps:
# a) Generate some random data for X and Y (use rnorm function in R)
# b) Check linearity by running lm(y~x) and get the coefficient
# c) Build Cost and gradient descent function
# d) The output of betas (ùõΩ0  + ùõΩ1) should be equal to the coefficients above
```

```{r a) Random Data Generation}
# Generates linear data with normal residuals
set.seed(123)
x <- rnorm(n = 30)

epsilon <- rnorm(n = 30)

y <- 5*x + 1 + epsilon
```

```{r Plots generated data, include = F}
plot(x, y)
```

```{r b) Checks linearity and coefficients use lm}
summary(lm(y ~ x))
```

```{r c) Defines a function for calculating cost function}
# Calculates the mean squared error for a simple linear regression model.
# @param x - a vector of the explainatory variable.
# @param y - a vector of the response variable.
# @param beta_0 - the intercept value for the current SLR model.
# @param beta_1 - the slope value for the current SLR model.
# @return - sum total mean squared error (y_hat - y)^2
slr_mse <- function(x, y, beta_0, beta_1){
    cost <- ((beta_1 * x + beta_0) - y)^2
    return(sum(cost))
}
```

```{r c) Calculates betas for SLR by gradient descent}
# Calculates the slope and intercept values for SLR
# or simple linear regression.
# @param x - a vector of the explanatory variable.
# @param y - a vector of the response variable.
# @param alpha - the learning rate.
# @return betas - a vector containing the calculated betas.
slr_gradient_desc <- function(x, y, alpha){

    # Summary statistic calculations.
    # Helps to calculate the gradient faster.
    x_bar <- mean(x)
    y_bar <- mean(y)
    xy_bar <- mean(x*y)
    x_sqbar <- mean(x^2)

    # initial guess for beta_0 and beta_1.
    beta_0 <- y_bar
    beta_1 <- 0

    # A counter to determine is the error is unchanging.
    # This is the Loop-Control-Variable (LCV).
    count_same <- 0

    # Iterate 100 times or until the cost remains unchanged for 10 iterations.
    for(i in 1:1000){

        # Stop the loop if the LCV >= 10.
        if(count_same >= 10){
            break
        }

        # Cost prior to beta adjustment.
        cost_start <- slr_mse(x, y, beta_0, beta_1)

        # Calculate gradient values.
        g_0 <- beta_0 + (beta_1 * x_bar) - y_bar
        g_1 <- (beta_0 * x_bar) + (beta_1 * x_sqbar) - xy_bar

        # Update betas.
        beta_0 <- beta_0 - (alpha * g_0)
        beta_1 <- beta_1 - (alpha * g_1)

        # If the cost is unchanged add 1 to the LCV.
        if(cost_start == slr_mse(x, y, beta_0, beta_1)){
            count_same <- count_same + 1
        }
    }

    return(c(beta_0 = round(beta_0, 4),
             beta_1 = round(beta_1, 4),
             iterations = i))
}
```

```{r d) Output calculated beta_0 and beta_1}
slr_gradient_desc(x, y, alpha = 0.1)
```

## Linear Model on Airbnb Data
```{r}
# Libraries
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
```

```{r notes on model building, include = F}
# Skewness in the predictors does matter as long as the residuals are
# normally distributed. This usually only concerns y; however,
# skewness in x can lead to influential points.
```

### Modeling Bulding
```{r Data import}
# Import raw data.
data_raw <- read.csv(file = "AB_NYC_2019.csv", stringsAsFactors = T,
                     header = T)
```

```{r Train/Test Split}
# 70/30 train/test data split
# Data was split prior to cleaning to prevent data leakage.
set.seed(123)
data_split <- initial_split(data_raw, prop = 0.7)

train_data <- training(data_split)
test_data <- testing(data_split)
```

```{r Initialize a recipe variable}
# Define a cleaning recipe from the data.
# Assigns price the role of response.
# Recipe is used to create a pipline that can works with new raw data
# assuming that the format matches the training data.
cleaning_recipe <- recipe(price ~ ., data = train_data)
```

```{r Add remove id variable step to recipe}
cleaning_recipe <- cleaning_recipe %>% step_rm(id, host_id, name, host_name)
```

```{r Add recode missing values to recipe}
cleaning_recipe <- cleaning_recipe %>%
    step_mutate(
        # numeric recodes
        latitude = ifelse(latitude == 0, NA, latitude),
        longitude = ifelse(longitude == 0, NA, longitude),
        minimum_nights = ifelse(minimum_nights == 0, NA, minimum_nights),

        #Factor recodes
        # It was done using replace to maintain factor coding
        room_type = replace(room_type,
            room_type == "" | room_type == " ", NA),
        neighbourhood_group = replace(neighbourhood_group,
            neighbourhood_group == "" | neighbourhood_group == " ",  NA),
        neighbourhood = replace(neighbourhood,
            neighbourhood == "" | neighbourhood == " ", NA),
        last_review = replace(last_review,
            last_review == "" | room_type == " ", NA)
    )
```

```{r Recode the response separately}
# Skip = T tell the fit to ignore the step if it can't be done.
# Necessary because we assume the test_data does not have the response.
cleaning_recipe <- cleaning_recipe %>%
                   step_mutate(price = ifelse(price == 0, NA, price), skip = T)
```
```{r Feature Engineering}
# Geodist (?)
cleaning_recipe <- cleaning_recipe %>%
    step_mutate(

            # Change NA reviews per month to be 0.
            reviews_per_month = ifelse(is.na(reviews_per_month),
                0, reviews_per_month),

            # Create a new variable last_review_year to reduce dimensionality.
            last_review_year = substring(last_review, 1, 4),

            # Modify NA last_review to be a new level "none".
            last_review_year = replace(last_review_year,
            is.na(last_review_year), "none"),

            # Cast to be factor.
            last_review_year = as.factor(last_review_year),

            # Remove old last review variable
            last_review = NULL
    )
```

```{r Check Skewness}
plot_skew <- train_data %>%
             select(where(is.numeric)) %>%
             select(-c(id, host_id)) %>%
             pivot_longer(cols = everything()) %>%
             ggplot(aes(value)) +
             geom_histogram() +
             facet_wrap(~name, scales = "free_x") +
             theme_bw() +
             labs(x = "", y = "Frequency")
```

```{r Add log transform and YeoJohnson to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   step_YeoJohnson(all_numeric_predictors()) %>%
                   # price was logged instead of YeoJohnson to maintain
                   # reversibility and interpretability.
                   step_log(price, skip = T)
```

```{r Add Normalize, Dummy Variables, and Imputation to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   step_normalize(all_numeric_predictors()) %>%
                   step_dummy(all_nominal_predictors()) %>%
                   step_impute_knn(everything(), neighbors = 10,
                                   impute_with = imp_vars(everything()),
                                   skip = T)
```

```{r Add filter out NZV and High CORR to recipe}
cleaning_recipe <- cleaning_recipe %>%
                   # removes predictor with less than 10% unique values and
                   # a greater than 95/5 ratio of most prevalent to next most
                   step_nzv(all_predictors()) %>%
                   step_corr(all_numeric_predictors())
```

```{r Bake at substage to check linearity assumption}
cleaning_recipe <- cleaning_recipe %>% prep(retain = T, verbose = T)
train_scatter <- bake(cleaning_recipe, new_data = NULL)
```

```{r Check Linearity}
# Matrix Scatterplot
plot_scatter <- train_scatter %>%
                select(-starts_with(c("neighbourhood",
                                      "room_type", "last_review"))) %>%
                pivot_longer(cols = -price) %>%
                ggplot(aes(x = value, y = price)) +
                geom_point() +
                facet_wrap(~name, scales = "free_x") +
                theme_bw() +
                labs(x = "", y = "ln(Price)")

plot_scatter
```

```{r Convert numerics into natural cubic splines}
cleaning_recipe <- cleaning_recipe %>%
                   # makes sure not to accidentally spline your response variable
                   step_ns(-starts_with(c("neighbourhood",
                                "room_type", "last_review", "price")),
                           deg_free = 3) %>%
                   # called again to remove overdetermined splines
                   step_corr(all_numeric_predictors())
```

```{r Summary of MLR Model Build}
mlr_mod <- linear_reg() %>% set_engine("lm")

mlr_wflow <- workflow() %>%
             add_model(mlr_mod) %>%
             add_recipe(cleaning_recipe)

mlr_wflow
```

```{r Summary of MLR Fit}
mlr_fit <- mlr_wflow %>%
           fit(data = train_data)

mlr_fit_lm <- mlr_fit %>% extract_fit_engine()

summary(mlr_fit_lm)

par(mfrow = c(1,2))
plot(mlr_fit_lm, which = 1:2)
```

```{r Check Influential Points}
library(MASS)
library(ggpubr)

##Influential Point Analysis ##

##Calculate Leverage, Studentized Residuals, and Cook's Distance.
Id <- 1:length(train_data$id)
Leverage <- hatvalues(mlr_fit_lm)
StudRes <- studres(mlr_fit_lm)
CookD <- cooks.distance(mlr_fit_lm)

inful_data <- cbind(Id, Leverage, StudRes, CookD)
inful_data <- as.data.frame(inful_data)

##Plots
##Leverage
lev <- ggplot(data = inful_data, aes(x = Id, y = Leverage)) + geom_point() +
            geom_hline(yintercept = 2 * length(mlr_fit_lm$coefficients) /
                length(inful_data$Id), col = "red") +
            labs(x = "Index") +
            theme_bw()

##Studentized Residuals
studres <- ggplot(data = inful_data, aes(x = Id, y = StudRes)) + geom_point() +
                geom_hline(yintercept = 2, col = "red") +
                geom_hline(yintercept = -2, col = "red") +
                labs(y = "Studentized Residuals", x = "Index") +
                theme_bw()

##Cooks distance
cooks <- ggplot(data = inful_data, aes(x = Id, y = CookD)) + geom_point() +
             geom_hline(yintercept = 1, col = "red") +
             labs(y = "Cook's Distance", x = "Index") +
             theme_bw()

inful <- ggarrange(lev, studres, cooks, ncol = 3, nrow = 1)

annotate_figure(inful, top = text_grob("Influential Point Analysis"))

```


```{r Back transform the prediction}
# Group prediction results raw v. training error v. testing error.
# metric r = Predicted / Actually
# Looking for the mean closest to 1 with the smallest spread.

# Extract transformed pricing variables
train_baked <- bake(prep(cleaning_recipe), new_data = NULL)

# Fresh = T tell the recipe to apply all the steps again
# training = test_data tell the recipe to treat the test_data
# as if it was the training data used in the preprocessing step
# this avoids the modeling problem of steps having to be skipped
# when the response is absent from the dataset.
test_baked <- bake(
    prep(cleaning_recipe, fresh = T, training = test_data, retain = T),
    new_data = NULL)

train_price <- train_baked %>% select(price)
test_price <- test_baked %>% select(price)
train_pred <- predict(mlr_fit, new_data = train_data)
test_pred <- predict(mlr_fit, new_data = test_data)

training_results <- cbind(train_price, train_pred)
testing_results <- cbind(test_price, test_pred)

training_results <- training_results %>%
                    mutate(ratio = price / .pred)

testing_results <- testing_results %>%
                    mutate(ratio = price / .pred)
```

```{r Model evaluation metrics}
train_rss <- sum((training_results$price - training_results$.pred)^2)
test_rss <- sum((testing_results$price - testing_results$.pred)^2)

train_n <- nrow(training_results)
test_n <- nrow(testing_results)
model_p <- length(coef(mlr_fit_lm)) - 1 #subtract 1 to ignore the intercept

train_tss <- var(training_results$price) * train_n
test_tss <- var(testing_results$price) * test_n

train_rsq <- 1 - (train_rss / train_tss)
test_rsq <- 1 - (test_rss / test_tss)

train_adj_rsq <- 1 - (1 - train_rsq) * (train_n - 1) /
                 (train_n - model_p - 1)
test_adj_rsq <- 1 - (1 - test_rsq) * (test_n - 1) /
                 (test_n - model_p - 1)

hist(training_results$ratio)
hist(testing_results$ratio)

train_ratio_mean <- mean(training_results$ratio)
train_ratio_sd <- sd(training_results$ratio)
test_ratio_mean <- mean(testing_results$ratio)
test_ratio_sd <- sd(testing_results$ratio)
```

```{r Variable importance}
library(vip)
mlr_fit %>% extract_fit_parsnip %>% vip()
```