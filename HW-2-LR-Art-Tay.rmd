---
title: "Linear Regression"
author: "Art Tay"
output: pdf_document
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
```

## Gradient Derivation
a)
\begin{align*}
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2(\beta_0 + \beta_1 x^{(i)} - y^{(i)}) \cdot 1
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m} \beta_0 +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} -
\frac{1}{m} \sum_{i = 0}^{m} y^{(i)}
\\
&=
\beta_0 + \beta_1 \bar x - \bar y
\end{align*}
b)
\begin{align*}
\dfrac{\partial}{\partial \beta_1}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\dfrac{\partial}{\partial \beta_1}
\left[
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2 x^{(i)}(\beta_0 + \beta_1 x^{(i)} - y^{(i)})
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m}
\left[
\beta_0 x^{(i)}+ \beta_1 {x^{(i)}}^2 - x^{(i)}y^{(i)}
\right]
\\
&=
\beta_0 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m}  {x^{(i)}}^2 -
\frac{1}{m} \sum_{i = 0}^{m} x^{(i)}y^{(i)}
\\
&=
\beta_0 \bar x +
\beta_1 \bar{x^2} -
\bar {xy}
\end{align*}

## Linear Regression by Gradient Decent
```{r Problem Statement 2, include = F}
# Linear regression by gradient descent in R
# The purpose of this exercise is to test what is above and implement that in R.
# Steps:
# a) Generate some random data for X and Y (use rnorm function in R)
# b) Check linearity by running lm(y~x) and get the coefficient
# c) Build Cost and gradient descent function
# d) The output of betas (𝛽0  + 𝛽1) should be equal to the coefficients above
```

```{r}
# Generates linear data with normal residuals
set.seed(123)
x <- rnorm(n = 30)

epsilon <- rnorm(n = 30)

y <- 5*x + 1 + epsilon

plot(x,y)
```

```{r}
ols_cost <- function(x, y, beta_0, beta_1){
    cost <- (y - (beta_1 * x + beta_0))^2
    return(sum(cost))
}
```

```{r}
slr_gradient_desc <- function(x, y, alpha){

    # Summary statistic calculations.
    # Helps to calculate the gradient faster.
    x_bar <- mean(x)
    y_bar <- mean(y)
    xy_bar <- mean(x*y)
    x_sqbar <- mean(x^2)

    # initial guess for beta_0 and beta_1
    beta_0 <- y_bar
    beta_1 <- 0

    cost_0 <- ols_cost(x, y, beta_0, beta_1)

    # Dataframe to store results.
    df_results <- c(beta_0 = beta_0,
                    beta_1 = beta_1,
                    error = cost_0)

    # A counter to determine is the error is unchanging.
    count_same <- 0

    while(count_same < 10){

        # Cost prior to beta adjustment.
        cost_start <- ols_cost(x, y, beta_0, beta_1)

        #print(cost_start)

        #print(c(beta_0, beta_1))

        # Calculate gradient values.
        g_0 <- beta_0 + (beta_1 * x_bar) - y_bar
        g_1 <- (beta_0 * x_bar) + (beta_1 * x_sqbar) - xy_bar

        # Update betas.
        beta_0 <- beta_0 - (alpha * g_0)
        beta_1 <- beta_1 - (alpha * g_1)

        # Calculate new cost.
        cost_after <- ols_cost(x, y, beta_0, beta_1)

        #print(cost_after)

        #print(c(beta_0, beta_1))

        # Check cost relation.
        #if(cost_start < cost_after) {
            #return("Bad alpha, over shot minimum! Lower alpha and try again")
        if(cost_start == cost_after) {
            df_results <- rbind(df_results,
                                c(beta_0, beta_1, cost_after))
            count_same <- count_same + 1
        } else {
            df_results <- rbind(df_results,
                                c(beta_0, beta_1, cost_after))
        }
    }
    return(df_results)
}
```

## Test
```{r}
test_1 <- slr_gradient_desc(x, y, alpha = 0.01)
```

```{r, include = F}
#beta_0 <- y_bar
#beta_1 <- 0
#beta_1s <- c()

#for(i in 1:1000){
    ## Summary statistic calculations.
    ## Helps to calculate the gradient faster.
    #x_bar <- mean(x)
    #y_bar <- mean(y)
    #xy_bar <- mean(x*y)
    #x_sqbar <- mean(x^2)

    ## initial guess for beta_0 and beta_1

    #g_0 <- beta_0 + (beta_1 * x_bar) - y_bar
    #g_1 <- (beta_0 * x_bar) + (beta_1 * x_sqbar) - xy_bar

    ## Update betas.
    #beta_0 <- beta_0 - (0.01 * g_0)
    #beta_1 <- beta_1 - (0.01 * g_1)

    #beta_1s <- append(beta_1s, beta_1)
#}
```

```{r}
# gradient testing

# Summary statistic calculations.
x_bar <- mean(x)
y_bar <- mean(y)
xy_bar <- mean(x*y)
x_sqbar <- mean(x^2)

# Betas to plot
beta_0s <- seq(from = -10, to = 10, by = .1)
beta_1s <- seq(from = -10, to = 10, by = .1)

g_0 <- beta_0s + (beta_1s * x_bar) - y_bar
g_1 <- (beta_0s * x_bar) + (beta_1s * x_sqbar) - xy_bar

par(mfrow = c(1,2))
plot(beta_0s, g_0)
abline(h = 0, v = 1)
plot(beta_1s, g_1)
abline(h = 0, v = 5)
```

```{r}
costs <- c()

for(i in seq(from = 1, to = length(beta_0s))){
    costs <- append(costs, ols_cost(x, y, beta_0s[i], beta_1s[i]))
}

plot(beta_1s, costs)
abline(h = min(costs), v = 5)
```

## Linear Model on Airbnb Data
