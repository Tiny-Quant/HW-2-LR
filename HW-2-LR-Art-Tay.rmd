---
title: "Linear Regression"
author: "Art Tay"
output: pdf_document
---

```{r setup, include=FALSE}
##Setup code
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

# Libraries
library(tidyverse)
```

## Gradient Derivation
a)
\begin{align*}
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\dfrac{\partial}{\partial \beta_0}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2(\beta_0 + \beta_1 x^{(i)} - y^{(i)}) \cdot 1
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m} \beta_0 +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} -
\frac{1}{m} \sum_{i = 0}^{m} y^{(i)}
\\
&=
\beta_0 + \beta_1 \bar x - \bar y
\end{align*}
b)
\begin{align*}
\dfrac{\partial}{\partial \beta_1}
\left[
\frac{1}{2m} \sum_{i = 0}^{m}
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\dfrac{\partial}{\partial \beta_1}
\left[
(\beta_0 + \beta_1 x^{(i)} - y^{(i)})^2
\right]
\\
&=
\frac{1}{2m} \sum_{i = 0}^{m}
\left[
2 x^{(i)}(\beta_0 + \beta_1 x^{(i)} - y^{(i)})
\right]
\\
&=
\frac{1}{m} \sum_{i = 0}^{m}
\left[
\beta_0 x^{(i)}+ \beta_1 {x^{(i)}}^2 - x^{(i)}y^{(i)}
\right]
\\
&=
\beta_0 \frac{1}{m} \sum_{i = 0}^{m} x^{(i)} +
\beta_1 \frac{1}{m} \sum_{i = 0}^{m}  {x^{(i)}}^2 -
\frac{1}{m} \sum_{i = 0}^{m} x^{(i)}y^{(i)}
\\
&=
\beta_0 \bar x +
\beta_1 \bar{x^2} -
\bar {xy}
\end{align*}

## Linear Regression by Gradient Decent
```{r Problem Statement 2, include = F}
# Linear regression by gradient descent in R
# The purpose of this exercise is to test what is above and implement that in R.
# Steps:
# a) Generate some random data for X and Y (use rnorm function in R)
# b) Check linearity by running lm(y~x) and get the coefficient
# c) Build Cost and gradient descent function
# d) The output of betas (ğ›½0  + ğ›½1) should be equal to the coefficients above
```

```{r a) Random Data Generation}
# Generates linear data with normal residuals
set.seed(123)
x <- rnorm(n = 30)

epsilon <- rnorm(n = 30)

y <- 5*x + 1 + epsilon
```

```{r Plots generated data, include = F}
plot(x, y)
```

```{r b) Checks linearity and coefficients use lm}
summary(lm(y ~ x))
```

```{r c) Defines a function for calculating cost function}
# Calculates the mean squared error for a simple linear regression model.
# @param x - a vector of the explainatory variable.
# @param y - a vector of the response variable.
# @param beta_0 - the intercept value for the current SLR model.
# @param beta_1 - the slope value for the current SLR model.
# @return - sum total mean squared error (y_hat - y)^2
slr_mse <- function(x, y, beta_0, beta_1){
    cost <- ((beta_1 * x + beta_0) - y)^2
    return(sum(cost))
}
```

```{r c) Calculates betas for SLR by gradient descent}
# Calculates the slope and intercept values for SLR
# or simple linear regression.
# @param x - a vector of the explanatory variable.
# @param y - a vector of the response variable.
# @param alpha - the learning rate.
# @return betas - a vector containing the calculated betas.
slr_gradient_desc <- function(x, y, alpha){

    # Summary statistic calculations.
    # Helps to calculate the gradient faster.
    x_bar <- mean(x)
    y_bar <- mean(y)
    xy_bar <- mean(x*y)
    x_sqbar <- mean(x^2)

    # initial guess for beta_0 and beta_1.
    beta_0 <- y_bar
    beta_1 <- 0

    # A counter to determine is the error is unchanging.
    # This is the Loop-Control-Variable (LCV).
    count_same <- 0

    # Iterate 100 times or until the cost remains unchanged for 10 iterations.
    for(i in 1:1000){

        # Stop the loop if the LCV >= 10.
        if(count_same >= 10){
            break
        }

        # Cost prior to beta adjustment.
        cost_start <- slr_mse(x, y, beta_0, beta_1)

        # Calculate gradient values.
        g_0 <- beta_0 + (beta_1 * x_bar) - y_bar
        g_1 <- (beta_0 * x_bar) + (beta_1 * x_sqbar) - xy_bar

        # Update betas.
        beta_0 <- beta_0 - (alpha * g_0)
        beta_1 <- beta_1 - (alpha * g_1)

        # If the cost is unchanged add 1 to the LCV.
        if(cost_start == slr_mse(x, y, beta_0, beta_1)){
            count_same <- count_same + 1
        }
    }

    return(c(beta_0 = round(beta_0, 4),
             beta_1 = round(beta_1, 4),
             iterations = i))
}
```

```{r d) Output calculated beta_0 and beta_1}
slr_gradient_desc(x, y, alpha = 0.1)
```

## Linear Model on Airbnb Data
```{r notes on model building, include = F}
# Skewness in the predictors does matter as long as the residuals are
# normally distributed. This usually only concerns y; however,
# skewness in x can lead to influential points.
```

## Data Cleaning
